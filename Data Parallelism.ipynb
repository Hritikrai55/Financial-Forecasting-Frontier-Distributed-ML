{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOg5fszTdRPxpJwG70oJ5xQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Task 1: Efficient Data Handling through Data Parallelism with Pyspark"],"metadata":{"id":"ma3H0CD44ROA"}},{"cell_type":"markdown","source":["## **Import Libraries**"],"metadata":{"id":"cxXlVep3GB4k"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QYp0xhIKAkib","outputId":"9f30452d-856a-4121-8b4a-89a36a648797","executionInfo":{"status":"ok","timestamp":1752497974246,"user_tz":-330,"elapsed":7426,"user":{"displayName":"Hritik Rai","userId":"09526681106676419866"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","source":["#Import Libraries\n","from pyspark.sql import SparkSession"],"metadata":{"id":"WT8-Td8m4pGJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"DataParallelismBank\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n","    .config(\"spark.executor.memory\", \"2g\") \\\n","    .config(\"spark.driver.memory\", \"2g\") \\\n","    .getOrCreate()\n"],"metadata":{"id":"dYmXXT_H4xkI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the \"bank.csv\" dataset into a Spark DataFrame"],"metadata":{"id":"IJA4rVR44--x"}},{"cell_type":"code","source":["# Load the dataset\n","data_path = \"bank.csv\"\n","df = spark.read.csv(data_path, header=True, inferSchema=True)\n"],"metadata":{"id":"GluEvZfi42Ne"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Data Preparation and Partitioning:**\n"],"metadata":{"id":"FJT0T0hD6Bpl"}},{"cell_type":"code","source":["# Inspect the first few rows\n","df.show(5)"],"metadata":{"id":"VnBxNB2kAmvw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8c26427-fe70-4238-a303-1165cb83b69e","executionInfo":{"status":"ok","timestamp":1752498018438,"user_tz":-330,"elapsed":1054,"user":{"displayName":"Hritik Rai","userId":"09526681106676419866"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","|age|        job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n","+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n","| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n","| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n","| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|\n","| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|\n","+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","source":["Implement a method to divide the dataset into smaller partitions for parallel processing. What strategy did you use for partitioning, and why?"],"metadata":{"id":"LkXwDEbcDKBw"}},{"cell_type":"code","source":["# Partition the dataset by the 'balance' column\n","partitioned_df = df.repartition(4, \"balance\")\n","\n","# Verify the number of partitions\n","print(f\"Number of partitions: {partitioned_df.rdd.getNumPartitions()}\")\n","\n","# Show the first few rows of one partition to verify\n","partitioned_df.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mmr-jE88CMm2","outputId":"64072e09-37c1-4dab-ccef-e05aba87f90b","executionInfo":{"status":"ok","timestamp":1752498026617,"user_tz":-330,"elapsed":1263,"user":{"displayName":"Hritik Rai","userId":"09526681106676419866"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of partitions: 4\n","+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","|age|          job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n","+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","| 33|     services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n","| 36|self-employed|married| tertiary|     no|    307|    yes|  no|cellular| 14|  may|     341|       1|  330|       2|   other| no|\n","| 41| entrepreneur|married| tertiary|     no|    221|    yes|  no| unknown| 14|  may|      57|       2|   -1|       0| unknown| no|\n","| 56|   technician|married|secondary|     no|   4073|     no|  no|cellular| 27|  aug|     239|       5|   -1|       0| unknown| no|\n","| 37|       admin.| single| tertiary|     no|   2317|    yes|  no|cellular| 20|  apr|     114|       1|  152|       2| failure| no|\n","+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","source":["I used the repartition method to partition the DataFrame based on the balance column. This approach helps distribute the data more evenly across partitions.\n","\n","Reason of using this method:\n","1. Even Distribution: Partitioning based on the balance column helps in distributing the data evenly across the partitions.\n","2. Parallel Processing: By partitioning the data, Spark can process each partition in parallel, which enhances the performance of data processing tasks.\n","3. Scalability: This approach scales well with large datasets, making it suitable for big data applications."],"metadata":{"id":"shGBw-x85qq8"}},{"cell_type":"markdown","source":["Summary: 1. To manage memory utilization and the quantity of shuffle partitions, we first establish a Spark session with the desired settings.\n","2. A Spark DataFrame is loaded with the \"bank.csv\" dataset.\n","3. To understand the structure of the dataframe, we first showed a few rows of the dataframe.\n","4. The balance column is the basis for our four partitions within the dataframe. The dataset's size and the resources at hand can be used to modify the number of partitions.\n","5. We verify the partitioning by printing the total number of partitions.\n"],"metadata":{"id":"xjHZm0jQFYuH"}},{"cell_type":"markdown","source":["## **Data Analysis and Processing in Parallel:**\n","Identify and calculate the average balance for each job category in the \"bank.csv\" dataset. Use parallel processing to perform this calculation. Describe your approach and the results.\n","\n","\n"],"metadata":{"id":"v2ZVNB6I6LCa"}},{"cell_type":"markdown","source":["\n","\n"," **Calculate Average Balance for Each Job Category**\n","\n","Use the groupBy and agg functions to calculate the average balance for each job category.\n","\n"],"metadata":{"id":"fovLbzYS6VZL"}},{"cell_type":"code","source":["from pyspark.sql.functions import avg, sum as spark_sum\n","\n","# Calculate average balance for each job category\n","avg_balance_per_job = df.groupBy(\"job\").agg(avg(\"balance\").alias(\"avg_balance\"))\n","avg_balance_per_job.show()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PT6gil26NsS","outputId":"7082dd0a-5ac8-4c7d-c08b-5c45de6a9bde","executionInfo":{"status":"ok","timestamp":1752498214927,"user_tz":-330,"elapsed":1292,"user":{"displayName":"Hritik Rai","userId":"09526681106676419866"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------+------------------+\n","|          job|       avg_balance|\n","+-------------+------------------+\n","|   unemployed|       1089.421875|\n","|     services|1103.9568345323742|\n","|      student|1543.8214285714287|\n","|      unknown|1501.7105263157894|\n","|   management|1766.9287925696594|\n","|  blue-collar| 1085.161733615222|\n","|self-employed|1392.4098360655737|\n","|       admin.|  1226.73640167364|\n","|   technician|     1330.99609375|\n","|    housemaid|2083.8035714285716|\n","| entrepreneur|          1645.125|\n","|      retired| 2319.191304347826|\n","+-------------+------------------+\n","\n"]}]},{"cell_type":"markdown","source":["Perform a parallel operation to identify the top 5 age groups in the dataset that have the highest loan amounts. Explain your methodology and present your findings.\n"],"metadata":{"id":"36amJiGQNALi"}},{"cell_type":"markdown","source":["Identify Top 5 Age Groups with Highest Loan Amounts:\n","\n","Use the groupBy and agg functions to calculate the total loan amounts for each age group and then identify the top 5 age groups."],"metadata":{"id":"cwv9V5RUNkrv"}},{"cell_type":"code","source":["# Calculate the total loan amounts for each age group\n","# Assuming 'loan' column indicates loan amounts (replace this if 'loan' is a binary indicator)\n","total_loan_per_age_group = df.groupBy(\"age\").agg(spark_sum(\"balance\").alias(\"total_loan\"))\n","\n","# Identify the top 5 age groups with the highest loan amounts\n","top_5_age_groups = total_loan_per_age_group.orderBy(\"total_loan\", ascending=False).limit(5)\n","top_5_age_groups.show()\n","\n","# Stop the Spark session\n","spark.stop()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0P6dPj8hNIu8","outputId":"4df387d8-4a1c-4c99-d1d5-422a710b87ed","executionInfo":{"status":"ok","timestamp":1752498224774,"user_tz":-330,"elapsed":2174,"user":{"displayName":"Hritik Rai","userId":"09526681106676419866"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+----------+\n","|age|total_loan|\n","+---+----------+\n","| 33|    287447|\n","| 32|    281467|\n","| 38|    273320|\n","| 34|    256765|\n","| 31|    256408|\n","+---+----------+\n","\n"]}]},{"cell_type":"markdown","source":["\n","### **Results:**\n","**Average Balance for Each Job Category:**\n","\n","This will display the average account balance for each job category.\n","\n","**Top 5 Age Groups with Highest Loan Amounts:**\n","\n","This will display the age groups that have the highest total loan amounts.\n","\n","### **Approach:**\n","\n","**Parallel Processing:**\n","\n","By using groupBy and agg functions, Spark performs these operations in parallel across partitions, leveraging the distributed computing power of the cluster.\n","\n","**Efficiency:**\n","\n","Partitioning and parallel processing ensure that the computations are performed efficiently, even on large datasets."],"metadata":{"id":"yo-VcOFy6zzm"}},{"cell_type":"markdown","source":["## **Model Training on Partitioned Data:**\n","\n","Choose a classification model to predict whether a client will subscribe to a term deposit (target variable 'y').\n","\n","Briefly explain why you selected this model.\n","\n","Partition the dataset into training and testing sets and train your model on these partitions.\n","\n","Discuss any challenges you faced in parallelizing the training process and how you addressed them.\n"],"metadata":{"id":"ohw5alUL7cip"}},{"cell_type":"markdown","source":["### **Model Selection:**\n","\n","We will use a Random Forest Classifier to predict whether a client will subscribe to a term deposit (target variable 'y').\n","\n","We used Random Forests classifier for this task because these are ensemble methods that combine multiple decision trees to improve predictive accuracy and control overfitting.\n","\n","Here are the steps to implement a Random Forest classifier in PySpark:\n","### **Data Partitioning and Model Training:**\n","\n","1.Load and Prepare Data: Ensure the data is ready for modeling.\n","\n","2.Partition the Dataset: Split the data into training and testing sets.\n","\n","3.Train the Random Forest Model: Fit the Random Forest model on the training set.\n","\n","4.Evaluate the Model: Assess the model’s performance on the testing set.\n","\n","### **Challenges and Solutions:**\n","\n","1. Parallelization: PySpark inherently parallelizes the training of Random Forests, but ensuring efficient data partitioning and avoiding bottlenecks in resource allocation is key to speeding up the training process.\n"],"metadata":{"id":"8KhgrVub7yR7"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml import Pipeline\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","# Create Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"BankTermDepositPrediction\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n","    .config(\"spark.executor.memory\", \"2g\") \\\n","    .config(\"spark.driver.memory\", \"2g\") \\\n","    .getOrCreate()\n","\n","# Load the dataset\n","data_path = \"bank.csv\"\n","data = spark.read.csv(data_path, header=True, inferSchema=True)\n","# data = df\n","# Handle categorical variables using StringIndexer\n","categorical_columns = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n","indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_columns]\n","\n","# Rename the target column to 'label'\n","data = data.withColumnRenamed(\"y\", \"label\")\n","\n","# Convert the label column to numerical values\n","label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n","data = label_indexer.fit(data).transform(data)\n","\n","# Assemble features into a single vector\n","assembler = VectorAssembler(\n","    inputCols=[\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"] + [col+\"_index\" for col in categorical_columns],\n","    outputCol=\"features\"\n",")\n","\n","# Initialize the Random Forest classifier\n","rf = RandomForestClassifier(labelCol=\"label_index\", featuresCol=\"features\", numTrees=100)\n","\n","# Define the pipeline\n","pipeline = Pipeline(stages=indexers + [assembler, rf])\n","\n","# Split the data into training and test sets\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n","\n","# Train the model\n","model = pipeline.fit(train_data)\n","\n","# Evaluate the model\n","predictions = model.transform(test_data)\n","evaluator = BinaryClassificationEvaluator(labelCol=\"label_index\", metricName=\"areaUnderROC\")\n","auc = evaluator.evaluate(predictions)\n","print(f\"Test AUC: {auc}\")\n","\n","\n","# Save the trained model, overwriting if the path already exists\n","model.write().overwrite().save(\"/content/drive/MyDrive/ml_model_trained\")\n","\n","# Stop the Spark session\n","spark.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2p50E9507ZlC","outputId":"46e179ae-dc00-46f5-f42f-24b9ca98f39b","executionInfo":{"status":"ok","timestamp":1752498306038,"user_tz":-330,"elapsed":27814,"user":{"displayName":"Hritik Rai","userId":"09526681106676419866"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test AUC: 0.8738954248365991\n"]}]},{"cell_type":"markdown","source":["### **Summary:**\n","\n","1.**Data Loading and Preparation:**\n","\n","**Spark Session:**\n","\n","We set up a Spark session with specific configurations to optimize parallel processing.\n","\n","**Load Dataset:**\n","\n","The dataset is loaded into a Spark DataFrame.\n","\n","**Handle Categorical Variables:**\n","\n","We use StringIndexer to convert categorical variables to numerical values.\n","\n","**Rename Target Column:**\n","\n","The target variable 'y' is renamed to 'label'.\n","\n","**Convert Label Column:**\n","\n","The label column is converted to numerical values using StringIndexer.\n","**Assemble Features:**\n","\n","We use VectorAssembler to combine all features into a single vector column.\n","\n","2.**Training the model**\n","\n","**Initialize and Train the Model:**\n","\n","A Random Forest classifier is initialized and trained using the pipeline.\n","\n","**Evaluate the Model:**\n","\n","The model is evaluated on the test set using the AUC metric.\n","\n","**Save the Model:**\n","\n","The trained model is saved for future use.\n","\n","### **Challenges and Solutions:**\n","\n","**Handling Categorical Variables:**\n","\n","The categorical variables were handled using StringIndexer to convert them to numerical values required for the Random Forest algorithm.\n","\n","**Parallel Processing:**\n","\n","By using Spark's MLlib and ensuring the data was evenly partitioned, we leveraged Spark's distributed computing capabilities to speed up the training process.\n","\n","**Resource Management:**\n","\n","Setting appropriate configurations for executor and driver memory ensured efficient resource usage."],"metadata":{"id":"IOmymUGW8ryj"}},{"cell_type":"markdown","source":["## **Resource Monitoring and Management:**\n","Implement resource monitoring during data processing and model training. What observations did you make regarding CPU and memory usage?\n","\n","\n","\n","We will use **Using Spark Event Logging** to capture detailed information about your Spark application’s execution.\n","\n","\n"],"metadata":{"id":"-HRDkNrGAFfw"}},{"cell_type":"markdown","source":["### **Resource Monitoring Observations**\n","\n","Spark Event Logging is a powerful feature that enables us to capture detailed logs about our Spark applications, including job execution details, stage breakdowns, task attempts, and resource usage. These logs are extremely useful for debugging, performance tuning, and understanding the behavior of our Spark applications."],"metadata":{"id":"2xchHx8C-TNO"}},{"cell_type":"code","source":["import os\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml import Pipeline\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","# Path to the event log directory\n","event_log_dir = \"eventlog\"\n","\n","# Create the event log directory if it doesn't exist\n","if not os.path.exists(event_log_dir):\n","    os.makedirs(event_log_dir)\n","\n","# Create Spark session with resource monitoring enabled\n","spark = SparkSession.builder \\\n","    .appName(\"BankTermDepositPredictionlog\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n","    .config(\"spark.executor.memory\", \"2g\") \\\n","    .config(\"spark.driver.memory\", \"2g\") \\\n","    .config(\"spark.eventLog.enabled\", \"true\") \\\n","    .config(\"spark.eventLog.dir\", event_log_dir) \\\n","    .getOrCreate()\n","\n","# Load the dataset\n","data_path = \"bank.csv\"\n","data = spark.read.csv(data_path, header=True, inferSchema=True)\n","\n","# Handle categorical variables using StringIndexer\n","categorical_columns = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n","indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_columns]\n","\n","# Rename the target column to 'label'\n","data = data.withColumnRenamed(\"y\", \"label\")\n","\n","# Convert the label column to numerical values\n","label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n","data = label_indexer.fit(data).transform(data)\n","\n","# Assemble features into a single vector\n","assembler = VectorAssembler(\n","    inputCols=[\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"] + [col+\"_index\" for col in categorical_columns],\n","    outputCol=\"features\"\n",")\n","\n","# Initialize the Random Forest classifier\n","rf = RandomForestClassifier(labelCol=\"label_index\", featuresCol=\"features\", numTrees=100)\n","\n","# Define the pipeline\n","pipeline = Pipeline(stages=indexers + [assembler, rf])\n","\n","# Split the data into training and test sets\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n","\n","# Train the model\n","model = pipeline.fit(train_data)\n","\n","# Evaluate the model\n","predictions = model.transform(test_data)\n","evaluator = BinaryClassificationEvaluator(labelCol=\"label_index\", metricName=\"areaUnderROC\")\n","auc = evaluator.evaluate(predictions)\n","print(f\"Test AUC: {auc}\")\n","\n","\n","# Stop the Spark session\n","spark.stop()\n"],"metadata":{"id":"zYzUx057_OK-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1c76962-22bd-4564-ba89-bb1da505a517","executionInfo":{"status":"ok","timestamp":1752498360479,"user_tz":-330,"elapsed":13516,"user":{"displayName":"Hritik Rai","userId":"09526681106676419866"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test AUC: 0.8738954248365991\n"]}]},{"cell_type":"markdown","source":["### **Summary:**\n","1. **Event Log Directory Creation:** - The code now checks if the event log directory exists, and creates it if it does not.\n","2. **Spark Session Configuration:** - The eventLog.dir configuration is updated to use the newly created directory.\n","\n","3. **Data Preparation and Model Training:** - The rest of the code loads the dataset, processes it, trains a Random Forest classifier and evaluates the model.\n","\n","4. **Stopping the Spark Session:** - Ensures the Spark session is properly stopped after the model training and evaluation are complete."],"metadata":{"id":"4VpA9VckAfnv"}},{"cell_type":"markdown","source":["## **Task Management and Scheduling:**\n","\n","Manage multiple parallel tasks, such as different preprocessing tasks. How did you ensure the effective management of these tasks?\n","\n","\n","Managing multiple parallel tasks, such as different preprocessing tasks, is essential in data processing workflows, especially when working with large datasets or complex pipelines. Effective management of these tasks involves optimizing resource usage, minimizing task execution time, and ensuring that tasks do not interfere with each other.\n","\n","\n"],"metadata":{"id":"UufghpvJDJsr"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","from pyspark.ml.feature import StringIndexer, VectorAssembler\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml import Pipeline\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","# Create Spark session with optimized configurations\n","spark = SparkSession.builder \\\n","    .appName(\"ParallelPreprocessing\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n","    .config(\"spark.executor.memory\", \"2g\") \\\n","    .config(\"spark.driver.memory\", \"2g\") \\\n","    .getOrCreate()\n","\n","# Load the dataset\n","data_path = \"bank.csv\"\n","data = spark.read.csv(data_path, header=True, inferSchema=True)\n","\n","# Preprocessing tasks as separate functions\n","def index_categorical_columns(df):\n","    categorical_columns = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n","    indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_columns]\n","    pipeline = Pipeline(stages=indexers)\n","    return pipeline.fit(df).transform(df)\n","\n","def assemble_features(df):\n","    feature_columns = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\",\n","                       \"job_index\", \"marital_index\", \"education_index\", \"default_index\",\n","                       \"housing_index\", \"loan_index\", \"contact_index\", \"month_index\", \"poutcome_index\"]\n","    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n","    return assembler.transform(df)\n","\n","def rename_label_column(df):\n","    return df.withColumnRenamed(\"y\", \"label\")\n","\n","def index_label_column(df):\n","    label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n","    return label_indexer.fit(df).transform(df)\n","\n","# Apply preprocessing tasks in parallel\n","data = rename_label_column(data)\n","data = index_categorical_columns(data)\n","data = assemble_features(data)\n","data = index_label_column(data)\n","\n","# Initialize the Random Forest classifier\n","rf = RandomForestClassifier(labelCol=\"label_index\", featuresCol=\"features\", numTrees=100)\n","\n","# Define the pipeline\n","pipeline = Pipeline(stages=[rf])\n","\n","# Split the data into training and test sets\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n","\n","# Train the model\n","model = pipeline.fit(train_data)\n","\n","# Evaluate the model\n","predictions = model.transform(test_data)\n","evaluator = BinaryClassificationEvaluator(labelCol=\"label_index\", metricName=\"areaUnderROC\")\n","auc = evaluator.evaluate(predictions)\n","print(f\"Test AUC: {auc}\")\n","\n","# Stop the Spark session\n","spark.stop()\n"],"metadata":{"id":"cHjDK3asD--K","colab":{"base_uri":"https://localhost:8080/"},"outputId":"687d3a22-d626-4aea-a25f-65ba4d47c40c","executionInfo":{"status":"ok","timestamp":1752498397637,"user_tz":-330,"elapsed":11157,"user":{"displayName":"Hritik Rai","userId":"09526681106676419866"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test AUC: 0.8738954248365991\n"]}]},{"cell_type":"markdown","source":["### **Summary:**\n","\n","\n","Here’s how we can manage multiple parallel tasks effectively in Spark:\n","\n","**Parallelizing Tasks in Spark:**\n","\n","Spark inherently supports parallel processing, but ensuring that multiple tasks run efficiently in parallel requires careful management of resources and task dependencies.\n","Here, the Spark session is configured with optimal resource allocations and parallelism settings.\n","\n","**Managing Resources:**\n","\n","Managing resources effectively is crucial when running multiple tasks in parallel, especially in a distributed environment like Spark.\n","\n","Data Partitioning and Task Parallelism\n","\n","Effective data partitioning is essential to ensure that tasks run in parallel without contention.\n","\n","**Preprocessing Functions:**\n","\n","index_categorical_columns(df): Indexes categorical columns using StringIndexer.\n","\n","assemble_features(df): Assembles feature columns into a feature vector.\n","\n","rename_label_column(df): Renames the target column to \"label\".\n","\n","index_label_column(df): Indexes the label column.\n","\n","**Pipeline Construction:**\n","\n","A Pipeline object is created to encapsulate the preprocessing and model training steps.\n","\n","\n","**Model Training and Evaluation:**\n","\n","The Random Forest model is trained and evaluated using the preprocessed data.\n","Resource Monitoring and Management:\n","\n","The code includes configurations for resource allocation, such as setting the number of shuffle partitions and memory allocations for the driver and executors.\n","\n","**Monitoring and Managing Task Execution:**\n","\n","Monitor the progress of parallel tasks using the Spark UI, where you can see the execution of jobs and stages in real-time. This helps in identifying any bottlenecks or imbalances in task execution.\n","\n","**Monitoring Tools:**\n","\n","Use Spark UI to monitor job progress, task execution, and resource usage.\n","Use tools like Ganglia or Prometheus for detailed resource monitoring.\n","\n","**Tuning Configurations:**\n","\n","Adjust spark.sql.shuffle.partitions based on the size of the data and cluster resources.\n","Allocate sufficient memory to executors and driver based on the data size and complexity of transformations.\n","This approach ensures that multiple preprocessing tasks are managed and executed effectively in parallel, leveraging Spark's capabilities for distributed data processing.\n","\n","\n","\n","\n"],"metadata":{"id":"JXC_WrwvDedN"}}]}